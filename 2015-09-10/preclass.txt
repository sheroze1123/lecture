1. Look up the specs for the totient nodes. Having read the Roofline paper,
   draw a roofline diagram for one totient node (assuming only the
   host cores are used, for the moment).  How do things change with
   the addition of the two Phi boards?
    - Look at attached roofline_totient.png for one totient node without Phi boards.
    - Peak floating point performance Xeon E5-2620: 120GF/s
      Max meory bandwidth Xeon E5-2620 : 59GB/s
    - Look at attached roofline_totient_phi.png for one totient node with Phi boards.
    - Peak floating point performance Xeon E5-2620: 120GF/s
      Max meory bandwidth Xeon E5-2620 : 59GB/s
      Peak floating point performance Xeon Phi 5110p : 1010GF/s
      Max memory bandwith: 320GB/s


2. What is the difference between two cores and one core with
   hyperthreading?
    - Hyperthreading provides the illusion of having two cores by simulating two 
      logical cores using instruction level parallelism.
    - With hyperthreading, the two logical cores share physical resources and 
      with two cores, a communication medium is needed to share physical resources.

3. Do a Google search to find a picture of how memories are arranged
   on the Phi architecture.  Describe the setup briefly in your own
   words.  Is the memory access uniform or non-uniform?
    - The memory access in non-uniform. 
    - Each core has its own private L2 cache.
    - Memory controllers and PCIe logic is used to interface to the GGDR5 memory.

4. Consider the parallel dot product implementations suggested in the
   slides.  As a function of the number of processors, the size of the
   vectors, and typical time to send a message, can you predict the
   speedup associated with parallelizing a dot product computation?
   [Note that dot products have low arithmetic intensity -- the
    roofline model may be useful for reasoning about the peak
    performance for computing pieces of the dot product]
    - Let the number of processors be n, size of the vectors be D, time to send a message
      be m and the time per flop be t.
    - Then, the serial computation will take 2Dt time.
    - The parallel implmentation will take 2Dt/n + mn.
    - The speedup will then be 2Dt/( 2Dt/n + mn ).
