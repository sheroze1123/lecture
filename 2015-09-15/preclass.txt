For the questions regarding the Game of Life, you may want to refer
to the simple implementation included in the "life" subdirectory.
If you run "make glider", you can see a small example of running
the glider pattern for a few generations.

0.  How much time did you spend on this pre-class exercise, and when?
        Start time : 08:20pm September 14, 2015
        End time   : 10:00pm September 14, 2015
        Total time : 01:40

1.  What are one or two points that you found least clear in the
    9/15 slide decks (including the narration)?
        - What load balancing is precisely and how to leverage load balancing.
          (Concrete examples would help)
        - In particle systems slide deck, in slide 3 (forced example) I did
          not understand how exactly ode113 can be used to solve the problem.
        - In the passing particles slide (slide 12), it is hard to keep track of 
          the variables. Labels for the variables would help. And I do not understand
          how the inequality listed in the slide occured.
        - In particle mesh methods slide (slide 13), I did not understand
          how correction is computed.

2.  In the basic implementation provided, what size board for the Game
    of Life would fit in L3 cache for one of the totient nodes?  Add a
    timer to the code and run on the totient node.  How many cells per
    second can we update for a board that fits in L3 cache?  For a
    board that does not fit?
        - For each problem, we have to keep track of 2 boards. Each board has 
          (n+2)^2 elements. The size of the L3 cache of one of the totient nodes 
          is, 15MB. Each element in the board is a char* taking 4bytes of memory.
          Then, 4 * 2 * (n+2)^2 bytes = 15MB.
          n ~ 1370.
        - A 13670 x 1370 board would fit in the L3 cache of a totient node.
        - The totient node took 420 milliseconds to compute 50 generations with board size 500.
        - That is, 29.761 million cells/second. 
        - The totient node took 27510 milliseconds to compute 50 generations with board size 4000.
        - That is, 29.08 million cells/second.

3.  Assuming that we want to advance time by several generations,
    suggest a blocking strategy that would improve the operational
    intensity of the basic implementation.  Assume the board is
    not dilute, so that we must still consider every cell.  You may
    want to try your hand at implementing your strategy (though you
    need not spend too much time on it).
        - Split up the board into 4 pieces and use one core to compute one piece. Communicate
          the information on the borders to other cores and set up a synchronization barrier to
          advance a generation.

4.  Comment on what would be required to parallelize this code
    according to the domain decomposition strategy outlined in the
    slides.  Do you think you would see good speedups on one of
    the totient nodes?  Why or why not?
        - On larger boards when the board cannot fit in the caches, the domain decomposition 
          will help improve the performance.
        - On smaller boards, the communication overhead will make the decomposed code run slower.

5.  Suppose we want to compute long-range interactions between two
    sets of particles in parallel using the obvious n^2 algorithm in a
    shared-memory setting.  A naive implementation might look like

      struct particle_t {
          double x, y;
          double fx, fy;
      };

      // Update p1 with forces from interaction with p2
      void apply_forces(particle* p1, particle* p2);

      // Assume p is the index of the current processor,
      // part_idx[p] <= i < part_idx[p+1] is the range of
      // particles "owned" by processor p.
      //
      for (int i = part_idx[p]; i < part_idx[p+1]; ++i)
          for (int j = 0; j < npart; ++j)
              apply_forces(particle + i, particle + j);

    Based on what you know about memories and parallel architecture,
    do you see any features of this approach that are likely to lead
    to poor performance?

        - In a shared memory architecture, the apply_forces function would need
          to synchronize the updates to the particle using a locking mechanism.
        - The syncrhonization overhead in this naive implementation may cause poor performance. 
